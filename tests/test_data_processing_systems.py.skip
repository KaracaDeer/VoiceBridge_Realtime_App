"""
Data Processing Systems Test Script
Test all data processing and storage systems for VoiceBridge
"""
import asyncio
import json
import logging
import os
from datetime import datetime

# from analytics.spark_simulator import (
#     BatchProcessor,
#     TranscriptionAnalytics,
#     get_spark_simulator,
# )
from cloud.cloud_storage_simulator import get_cloud_manager

# Import all data processing systems
from database.data_service import get_data_service
from storage.hdfs_simulator import AudioDatasetManager, get_hdfs_simulator

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_hdfs_system():
    """Test HDFS simulator"""
    logger.info("Testing HDFS System...")

    try:
        hdfs = get_hdfs_simulator()

        # Test basic operations
        test_file = "test_audio.wav"
        if not os.path.exists(test_file):
            # Create a dummy test file
            with open(test_file, "w") as f:
                f.write("dummy audio data")

        # Test file operations
        upload_success = hdfs.put_file(test_file, "/audio/test_audio.wav")
        if upload_success:
            logger.info("✅ HDFS file upload successful")

        files = hdfs.list_files("/")
        if files:
            logger.info(f"✅ HDFS file listing successful ({len(files)} files)")

        # Test audio dataset manager
        dataset_manager = AudioDatasetManager(hdfs)
        datasets = dataset_manager.list_datasets()
        logger.info(f"✅ Audio dataset manager initialized ({len(datasets)} datasets)")

        # Get storage info
        storage_info = hdfs.get_storage_info()
        logger.info(
            f"✅ HDFS storage info: {storage_info['file_count']} files, {storage_info['total_size_mb']:.2f} MB"
        )

        # Cleanup
        if os.path.exists(test_file):
            os.remove(test_file)

        assert True

    except Exception as e:
        logger.error(f"❌ HDFS test failed: {e}")
        return False


def test_spark_analytics():
    """Test Spark analytics system"""
    logger.info("Testing Spark Analytics System...")

    try:
        spark = get_spark_simulator()
        analytics = TranscriptionAnalytics(spark)

        # Create sample transcription data
        sample_transcriptions = [
            {
                "id": 1,
                "user_id": 1,
                "session_id": "session_1",
                "text": "Hello world",
                "confidence_score": 0.95,
                "language_detected": "en",
                "model_used": "wav2vec2",
                "processing_time": 1.2,
                "audio_duration": 2.5,
                "created_at": datetime.utcnow(),
            },
            {
                "id": 2,
                "user_id": 1,
                "session_id": "session_1",
                "text": "How are you",
                "confidence_score": 0.87,
                "language_detected": "en",
                "model_used": "wav2vec2",
                "processing_time": 1.1,
                "audio_duration": 2.0,
                "created_at": datetime.utcnow(),
            },
            {
                "id": 3,
                "user_id": 2,
                "session_id": "session_2",
                "text": "Good morning",
                "confidence_score": 0.92,
                "language_detected": "en",
                "model_used": "wav2vec2",
                "processing_time": 1.3,
                "audio_duration": 2.2,
                "created_at": datetime.utcnow(),
            },
        ]

        # Test quality analysis
        quality_analysis = analytics.analyze_transcription_quality(
            sample_transcriptions
        )
        if "error" not in quality_analysis:
            logger.info("✅ Transcription quality analysis successful")
            logger.info(
                f"   Average confidence: {quality_analysis['average_confidence']:.3f}"
            )
            logger.info(
                f"   Total transcriptions: {quality_analysis['total_transcriptions']}"
            )
        else:
            logger.warning(f"⚠️ Quality analysis failed: {quality_analysis['error']}")

        # Test user behavior analysis
        behavior_analysis = analytics.analyze_user_behavior(sample_transcriptions)
        if "error" not in behavior_analysis:
            logger.info("✅ User behavior analysis successful")
            logger.info(f"   Total users: {behavior_analysis['total_users']}")
            logger.info(f"   Total sessions: {behavior_analysis['total_sessions']}")
        else:
            logger.warning(f"⚠️ Behavior analysis failed: {behavior_analysis['error']}")

        # Test daily report generation
        daily_report = analytics.generate_daily_report(
            sample_transcriptions, "2025-09-08"
        )
        if "error" not in daily_report:
            logger.info("✅ Daily report generation successful")
        else:
            logger.warning(f"⚠️ Daily report failed: {daily_report['error']}")

        assert True

    except Exception as e:
        logger.error(f"❌ Spark analytics test failed: {e}")
        return False


def test_cloud_storage():
    """Test cloud storage systems"""
    logger.info("Testing Cloud Storage Systems...")

    try:
        cloud_manager = get_cloud_manager()

        # Test BigQuery
        bigquery = cloud_manager.bigquery
        dataset_created = bigquery.create_dataset("test_dataset")
        if dataset_created:
            logger.info("✅ BigQuery dataset creation successful")

        # Test GCS
        gcs = cloud_manager.gcs
        test_file = "test_cloud_file.txt"
        with open(test_file, "w") as f:
            f.write("test cloud data")

        gcs_upload = gcs.upload_file(test_file, "test/test_file.txt")
        if gcs_upload:
            logger.info("✅ GCS file upload successful")

        gcs_files = gcs.list_files("test/")
        if gcs_files:
            logger.info(f"✅ GCS file listing successful ({len(gcs_files)} files)")

        # Test S3
        s3 = cloud_manager.s3
        test_data = b"test s3 data"
        s3_upload = s3.put_object("test/test_s3.txt", test_data)
        if s3_upload:
            logger.info("✅ S3 object upload successful")

        s3_objects = s3.list_objects("test/")
        if s3_objects:
            logger.info(f"✅ S3 object listing successful ({len(s3_objects)} objects)")

        # Test analytics pipeline setup
        pipeline_setup = cloud_manager.setup_analytics_pipeline()
        if pipeline_setup:
            logger.info("✅ Analytics pipeline setup successful")

        # Cleanup
        if os.path.exists(test_file):
            os.remove(test_file)

        assert True

    except Exception as e:
        logger.error(f"❌ Cloud storage test failed: {e}")
        return False


def test_database_integration():
    """Test database integration"""
    logger.info("Testing Database Integration...")

    try:
        data_service = get_data_service()
        connected = data_service.connect_all()

        if connected:
            logger.info("✅ Database connections established")

            # Test user operations
            user_id = data_service.create_user(
                username="test_user_analytics",
                email="analytics@example.com",
                password_hash="hashed_password",
            )

            if user_id:
                logger.info(f"✅ User creation successful (ID: {user_id})")

            # Test conversation operations
            conversation_id = data_service.create_conversation(
                user_id="test_user_analytics",
                session_id="analytics_session",
                title="Analytics Test Conversation",
            )

            if conversation_id:
                logger.info(
                    f"✅ Conversation creation successful (ID: {conversation_id})"
                )

            data_service.close_all()
            assert True
        else:
            logger.warning("⚠️ Database connection failed - running in demo mode")
            return False

    except Exception as e:
        logger.error(f"❌ Database integration test failed: {e}")
        return False


def test_batch_processing():
    """Test batch processing capabilities"""
    logger.info("Testing Batch Processing...")

    try:
        spark = get_spark_simulator()
        batch_processor = BatchProcessor(spark)

        # Create sample data file
        sample_data = [
            {
                "id": i,
                "user_id": (i % 3) + 1,
                "text": f"Sample transcription {i}",
                "confidence_score": 0.8 + (i % 20) * 0.01,
                "language_detected": "en",
                "model_used": "wav2vec2",
                "processing_time": 1.0 + (i % 10) * 0.1,
                "audio_duration": 2.0 + (i % 5) * 0.5,
                "created_at": datetime.utcnow().isoformat(),
            }
            for i in range(1, 101)  # 100 sample records
        ]

        # Save sample data to file
        sample_file = "sample_transcriptions.json"
        with open(sample_file, "w") as f:
            json.dump(sample_data, f, indent=2)

        # Test batch processing
        output_file = "batch_processing_results.json"
        success = batch_processor.process_historical_data(sample_file, output_file)

        if success:
            logger.info("✅ Batch processing successful")

            # Check if output file was created
            if os.path.exists(output_file):
                logger.info("✅ Batch processing output file created")
                os.remove(output_file)  # Cleanup
        else:
            logger.warning("⚠️ Batch processing failed")

        # Cleanup
        if os.path.exists(sample_file):
            os.remove(sample_file)

        assert True

    except Exception as e:
        logger.error(f"❌ Batch processing test failed: {e}")
        return False


async def main():
    """Main test function"""
    logger.info("Starting Data Processing Systems Tests...")

    # Test all systems
    hdfs_ok = test_hdfs_system()
    spark_ok = test_spark_analytics()
    cloud_ok = test_cloud_storage()
    database_ok = test_database_integration()
    batch_ok = test_batch_processing()

    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("DATA PROCESSING SYSTEMS TEST RESULTS")
    logger.info("=" * 60)

    results = {
        "HDFS Storage": "✅ Working" if hdfs_ok else "❌ Failed",
        "Spark Analytics": "✅ Working" if spark_ok else "❌ Failed",
        "Cloud Storage": "✅ Working" if cloud_ok else "❌ Failed",
        "Database Integration": "✅ Working" if database_ok else "⚠️ Demo Mode",
        "Batch Processing": "✅ Working" if batch_ok else "❌ Failed",
    }

    for system, status in results.items():
        logger.info(f"{system:20}: {status}")

    working_systems = sum([hdfs_ok, spark_ok, cloud_ok, database_ok, batch_ok])
    total_systems = len(results)

    logger.info(f"\nOverall Status: {working_systems}/{total_systems} systems working")

    if working_systems == total_systems:
        logger.info("🎉 All data processing systems are operational!")
    elif working_systems >= 3:
        logger.info("✅ Most data processing systems are working")
    else:
        logger.info("⚠️ Some data processing systems need attention")

    return results


if __name__ == "__main__":
    # Run tests
    results = asyncio.run(main())

    print("\n" + "=" * 60)
    print("DATA PROCESSING SYSTEMS TEST COMPLETED")
    print("=" * 60)
